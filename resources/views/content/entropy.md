# Entropy, what it is and why it matters

Definitions and examples:

Let's say there's a toothbrush. It, as anything else, consists of elementary particles, billions and billions of
them. Now let's imagine changing one of those particles, or removing it. How much would that affect how we see that
toothbrush, what would visibly change? Nothing, right? One elementary particle alone can't make a visible change. 2
particles? How about 100? If we continue increasing the number of affected particles, at some point the toothbrush
will look different or feel different when we touch it.

Right before changing the last particle, there was no visible difference, and that's the entropy - `how much we can
change and still get the "same" result?`

\
This is an interesting definition, but I think we can do better. Here's another one:

- _Sand castle_ - few ways to rearrange the sand grains without changing its state - **low entropy** - more ordered.
- _Sand pile_ - many ways to rearrange the sand grains without changing its state - **high entropy** - less ordered.

And some more:

- Entropy is a measure for how likely it is for the system to be in a certain configuration.
- Entropy is a measure of the number of possible states of a system.

And I like the framing of this one - _entropy always increases because it's overwhelmingly more likely that it will_.

---

The idea of entropy is fascinating, and you can see the increasing entropy effect in software development as well.
If you've worked on a project for a relatively long time, you know what I'm talking about.

To be continued...
